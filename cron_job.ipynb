{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrapDarazData():\n",
    "    from requests import get\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    import json \n",
    "    from time import sleep,time\n",
    "    from IPython.core.display import clear_output\n",
    "    import pandas as pd\n",
    "\n",
    "    id=[]\n",
    "    name=[]\n",
    "    brand=[]\n",
    "    image=[]\n",
    "    price=[]\n",
    "    originalPrice=[]\n",
    "    url=[]\n",
    "\n",
    "    base_url=\"https://www.daraz.com.np/smartphones/?page=\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'} \n",
    "\n",
    "    pages=13\n",
    "    start_time = time()\n",
    "    requests = 0\n",
    "\n",
    "    for page in range(1,pages+1):\n",
    "        sleep(0.1)\n",
    "        response = get(base_url+str(page),headers=headers)  \n",
    "        page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        script=page_html.findAll('script')[2].text\n",
    "        text=re.search('\"listItems\":\\[{.*}\\],\"re', script).group()\n",
    "        text=text.strip('\"listItems\": ,\"re')\n",
    "        text+='}]'\n",
    "        data=json.loads(text)\n",
    "\n",
    "        for d in data:\n",
    "            id.append(d['nid'])\n",
    "            name.append(d['name'])\n",
    "            brand.append(d['brandName'])\n",
    "            image.append(d['image'])\n",
    "            price.append(d['price'])        \n",
    "            try:\n",
    "                orp=d['originalPrice']\n",
    "            except Exception:\n",
    "                originalPrice.append(d['price'])\n",
    "            else:\n",
    "                originalPrice.append(orp) \n",
    "\n",
    "            url.append(d['productUrl'].replace('//','https://'))   \n",
    "    \n",
    "    darazData = pd.DataFrame({'id': id, \n",
    "                          'name': name, \n",
    "                          'brand':brand,\n",
    "                          'image':image, \n",
    "                          'price':price, \n",
    "                          'originalPrice':originalPrice,\n",
    "                          'url':url,\n",
    "                          'web_id':1}) \n",
    "    return darazData\n",
    "    \n",
    "def scrapNepBayData():\n",
    "    from requests import get\n",
    "    from bs4 import BeautifulSoup\n",
    "    from time import sleep\n",
    "    import pandas as pd\n",
    "\n",
    "    name=[]\n",
    "    brand=[]\n",
    "    url=[]\n",
    "    price=[]\n",
    "    originalPrice=[]\n",
    "\n",
    "    pages=6\n",
    "    base_url=\"https://nepbay.com/shopping/nepal/smart-phones-mobiles-in-nepal?page=\"\n",
    "\n",
    "    for page in range(1,pages+1):   \n",
    "        #make a get request    \n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        response = get(base_url+str(page),headers=headers)\n",
    "\n",
    "        # Pause the loop to reduce server overhead\n",
    "        sleep(0.1)\n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "        data=page_html.findAll(\"div\",{\"class\":\"ncs-ad-list\"}) \n",
    "\n",
    "        for d in data:\n",
    "            n=d.find(\"div\",{'class':'TGLBox'}).find('h4').text\n",
    "            name.append(n)\n",
    "            try:\n",
    "                b=n.split(' ')[0].strip()         \n",
    "            except Exception:\n",
    "                brand.append(n)\n",
    "            else:\n",
    "                brand.append(b)\n",
    "            pr=int(float(d.find(\"div\",{'class':'TGLBox'}).find('p').find('span').text.replace(',','')))\n",
    "            price.append(pr)\n",
    "            originalPrice.append(pr)\n",
    "            url.append(d.find(\"div\",{'class':'TGLBox'}).find('h4').find('a')['href']) \n",
    "        \n",
    "    nepbayData = pd.DataFrame({'name':name,\n",
    "                      'brand':brand,\n",
    "                      'price':price, \n",
    "                      'originalPrice':originalPrice,\n",
    "                      'url':url,\n",
    "                      'web_id':2})    \n",
    "    return nepbayData\n",
    "        \n",
    "        \n",
    "def scrapSastoDealData():\n",
    "    import time\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    import pandas as pd\n",
    "\n",
    "    brand=[]\n",
    "    name=[]\n",
    "    url=[]\n",
    "    image=[]\n",
    "    originalPrice=[]\n",
    "    price=[]\n",
    "\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.sastodeal.com/sastodeal/cta-mobiles-28?flag=setFilters&catalogueID=10&tct=4&apct=4&cts=28&emt=&searchkey=&defaultCategory=28&lvl=&bid=&size=&color=&sd=&bName=&spr=1000%20TO%20205000\")\n",
    "    elem = browser.find_element_by_tag_name(\"body\")\n",
    "\n",
    "    no_of_pagedowns = 90\n",
    "\n",
    "    while no_of_pagedowns:\n",
    "        elem.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(0.1)\n",
    "        no_of_pagedowns-=1\n",
    "\n",
    "    products=browser.find_elements_by_class_name('categorytDetailDiv')\n",
    "\n",
    "    for p in products:\n",
    "        t=p.find_element_by_class_name(\"title\").text\n",
    "        name.append(t)\n",
    "        url.append(p.find_element_by_class_name(\"fullWidth\").get_attribute(\"href\"))\n",
    "        p1=p.find_element_by_class_name(\"offer-price\").text.split()[1].replace(',','')\n",
    "        price.append(p1)\n",
    "        try:\n",
    "            p2=p.find_element_by_class_name(\"linethrough\").text.split()[1].replace(',','')\n",
    "        except Exception:\n",
    "            originalPrice.append(p1)\n",
    "        else:\n",
    "            originalPrice.append(p2)\n",
    "        brand.append(t.split(\" \")[0])\n",
    "\n",
    "    sastoDealData = pd.DataFrame({'name': name, \n",
    "                        'url': url,  \n",
    "                        'price':price, \n",
    "                        'originalPrice': originalPrice, \n",
    "                        'brand': brand,\n",
    "                        'web_id':3})\n",
    "    return sastoDealData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def namecleaner(name):\n",
    "    name=re.sub(r'\\(.*\\)', '', name)\n",
    "    name=re.sub(r'\\[.*\\]', '', name)\n",
    "    return name\n",
    "\n",
    "def scrapper():\n",
    "    darazData=scrapDarazData()\n",
    "    sleep(0.5)\n",
    "    nepBayData=scrapNepBayData()\n",
    "    sleep(0.5)\n",
    "    sastoDealData=scrapSastoDealData()    \n",
    "    \n",
    "    maindf = pd.DataFrame(columns=['id','name','url','image','price','originalPrice','brand','web_id'])\n",
    "    rows=[]\n",
    "    for i in range(len(sdf)):\n",
    "        ratio=[]\n",
    "        for j in range(len(ddf)):\n",
    "            ratio.append(fuzz.token_set_ratio(namecleaner(sdf['name'][i]),namecleaner(ddf['name'][j]))) \n",
    "        if(max(ratio)>=95):\n",
    "            k=ratio.index(max(ratio))\n",
    "            row=sdf.loc[i]\n",
    "            row['id']=ddf['id'][k]\n",
    "            rows.append(row)\n",
    "            rows.append(ddf.loc[k])\n",
    "            \n",
    "    df1=pd.DataFrame(rows)\n",
    "    df2=df1.copy()\n",
    "    df1=df1.drop_duplicates(['name'], keep='first')\n",
    "    df2=df2[df2.drop_duplicates().image.isnull()==False].reset_index(drop=True)\n",
    "    \n",
    "    import pymysql as sql\n",
    "    connection = sql.connect(host='localhost',user='root',password='deven123',database='shoplab')\n",
    "    \n",
    "    #data insertion for product table\n",
    "    for i in range(len(df1)):\n",
    "        cursor = connection.cursor()\n",
    "        id=np.str(df1['id'][i])\n",
    "        name=np.str(df1['name'][i])\n",
    "        brand=np.str(df1['brand'][i])\n",
    "        description=np.str(df1['description'][i])\n",
    "        image=np.str(df1['image'][i])\n",
    "        sql=(\"INSERT INTO products(id,name,brand,description,image) VALUES(%s,%s,%s,%s,%s)\")\n",
    "        val=(id,name,brand,description,image)\n",
    "        cursor.execute(sql,val)\n",
    "        connection.commit()\n",
    "    \n",
    "    #data insertion for prices table\n",
    "    for i in range(len(df2)):\n",
    "        cursor = connection.cursor()\n",
    "        productid=np.str(df2['id'][i])\n",
    "        webid=np.str(df2['web_id'][i])\n",
    "        price=np.str(df2['price'][i])\n",
    "        originalprice=np.str(df2['originalPrice'][i])\n",
    "        url=np.str(df2['url'][i])\n",
    "        sql=(\"INSERT INTO prices(productid,webid,price,originalprice,url) VALUES(%s,%s,%s,%s,%s)\")\n",
    "        val = (productid, webid,price,originalprice,url)\n",
    "        cursor.execute(sql, val)\n",
    "        connection.commit()\n",
    "\n",
    "scheduler = BlockingScheduler()\n",
    "scheduler.add_job(scrapper, 'interval', hours=12)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
